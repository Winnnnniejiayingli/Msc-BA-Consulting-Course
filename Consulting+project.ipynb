{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f230c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c1b9d47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\13719\\anaconda3\\envs\\base0\\lib\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\13719\\anaconda3\\envs\\base0\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set stopwords and initialize the WordNet Lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()'''\n",
    "\n",
    "# Function to preprocess and lemmatize text\n",
    "def preprocess_and_lemmatize(text):\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words if word not in stop_words]\n",
    "    # Join words back into a string\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7b2a193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def n_grams(document,n):\\n    ngrams_list=[]\\n    for document in documents:\\n        tokens = nltk.word_tokenize(document)\\n        ngrams_tokens = ngrams(tokens, n)\\n        ngrams_list.extend([' '.join(gram) for gram in ngrams_tokens])\\n    return ngrams_list\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def n_grams(document,n):\n",
    "    ngrams_list=[]\n",
    "    for document in documents:\n",
    "        tokens = nltk.word_tokenize(document)\n",
    "        ngrams_tokens = ngrams(tokens, n)\n",
    "        ngrams_list.extend([' '.join(gram) for gram in ngrams_tokens])\n",
    "    return ngrams_list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c64026fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_input=pd.read_excel(\"consulting dataset.xlsx\")[\"Input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "14425dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process(file):\n",
    "    global output_list\n",
    "    df_output=pd.read_excel(file)[['Output1','Output2','Output3','Output4','Output5','Output6']]\n",
    "    output_list=[df_output.iloc[i,j] for i in range(len(df_output)) for j in range(0,5)]\n",
    "    output_list=list(set(output_list))\n",
    "    output_list.remove(np.nan)\n",
    "    output_clean=[preprocess_and_lemmatize(x) for x in output_list]\n",
    "    return output_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df59ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_output=pd.read_excel(\"consulting dataset.xlsx\")[['Output1','Output2','Output3','Output4','Output5','Output6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80aa1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_list=[df_output.iloc[i,j] for i in range(len(df_output)) for j in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45a93786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_list=list(set(output_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8887dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_list.remove(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3aef78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_clean=[preprocess_and_lemmatize(x) for x in df_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "588df4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_clean=[preprocess_and_lemmatize(x) for x in output_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a02b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_based_model(input_text,output_all, num=7):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(output_all)\n",
    "    input_tfidf = tfidf_vectorizer.transform([input_text])\n",
    "    similarities = cosine_similarity(input_tfidf, tfidf_matrix)\n",
    "    most_similar_indices = similarities.argsort()[0][-num:].tolist()\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Output': [output_list[x] for x in most_similar_indices]\n",
    "    })\n",
    "    \n",
    "    # Display queried article details\n",
    "    print(\"=\"*30, \"Input\", \"=\"*30)\n",
    "    print(input_text)\n",
    "    \n",
    "    # Display recommended similar articles\n",
    "    print(\"\\n\", \"=\"*25, \"Output:\", \"=\"*23)\n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c975c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\13719\\anaconda3\\envs\\base0\\lib\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\13719\\anaconda3\\envs\\base0\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the most relevant items that align with your interests and goals:Course: \"Data Visualization with Python\" - This course teaches how to use Python to create visual representations of data, which aligns with the concept of visual analytics and using visual representation of data for decision making.\n",
      "============================== Input ==============================\n",
      "Course: \"Data Visualization with Python\" - This course teaches how to use Python to create visual representations of data, which aligns with the concept of visual analytics and using visual representation of data for decision making.\n",
      "\n",
      " ========================= Output: =======================\n",
      "                                              Output\n",
      "0  Data Visualization: IBM Academic provides a co...\n",
      "1  Data Visualization: IBM provides courses on da...\n",
      "2  Course Title: Data Visualization\\n   Descripti...\n",
      "3  \"Data Visualization\" course: This course cover...\n",
      "4  Data Visualization: IBM offers courses on data...\n",
      "5  Visual Analytics: This course explores the pri...\n",
      "6  Course: \"Data Visualization with Python\" - Thi...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import nltk\n",
    "    from gensim.models import KeyedVectors\n",
    "    import gensim.downloader as api\n",
    "    import gensim.downloader\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Download necessary NLTK data\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Set stopwords and initialize the WordNet Lemmatizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    \n",
    "    output_clean=read_process('consulting dataset.xlsx')\n",
    "    \n",
    "    input_text=input(\"Find the most relevant items that align with your interests and goals:\")\n",
    "    \n",
    "    print(tfidf_based_model(input_text,output_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc38dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
